{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ec3b5c-e5b1-46fb-8f6f-4fadf389cfa5",
   "metadata": {},
   "source": [
    "### EBA Q&A evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bae6bf3-46df-47a4-8898-3dbba82dd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145a074-3b1a-49ae-a9be-afe2417ab441",
   "metadata": {},
   "source": [
    "### A. Prepare prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a2e342-27b2-4779-84c4-6a2afe26b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rater prompt\n",
    "RATER_PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {question}\n",
    "************\n",
    "[Context]: {context}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "Rate the submission on a scale of 1 to 10.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192cc15a-9b0c-4d7f-b52f-ab24ff3c4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rater prompt\n",
    "ANSWER_PROMPT = \"\"\"\\\n",
    "You are a regulatory expert from a central bank who is reponsible for answering question coming from commercial banks.\n",
    "You will get a question and context which is required for providing the data.\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {question}\n",
    "************\n",
    "[Context]: {context}\n",
    "************\n",
    "[END DATA]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb668f9-cd4a-4e9c-8c9d-fab5ae5f1190",
   "metadata": {},
   "source": [
    "### B. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0873be81-14c7-4862-8d6d-54f4f45eca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_rater(question, context, output, expected):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": RATER_PROMPT.format(question=question, context=context, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f058ecfc-6d31-4926-a5ff-8616ff4bd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    o1_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": ANSWER_PROMPT.format(question=question, context=context),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = wrap_long_lines(o1_response.choices[0].message.content, width=100)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d2b022-7a82-42fc-a214-58e1b554cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_long_lines(text, width=100):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = []\n",
    "    for line in lines:\n",
    "        wrapped_lines.extend(textwrap.wrap(line, width=width))\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fb3e1-1da8-497d-9b2b-d526575f4ab9",
   "metadata": {},
   "source": [
    "### C. Answer and rate questions from the EBA Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4604c4-5efa-47c2-95dd-687d482da7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve questions, context and answers from the EBA Q&A\n",
    "with open(\"qa.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5061c092-b107-4c23-b630-a48bc8681cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for item in data:\n",
    "    question = item[\"question\"]\n",
    "    context = item[\"context\"]\n",
    "    expected = item.get(\"expected\", \"\")\n",
    "    output = answer_question(question, context)\n",
    "    rating = numeric_rater(question, context, output, expected)\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"output\": output,\n",
    "        \"expected\": expected,\n",
    "        \"rating\": rating\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be8287f-a8f7-43db-8000-0af9465306aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>output</th>\n",
       "      <th>expected</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>For the treatment of cured defaulted exposures...</td>\n",
       "      <td>Source 1: CRR Article 178 &gt; Default of an obli...</td>\n",
       "      <td>Yes, a probation period of 90 days with no def...</td>\n",
       "      <td>Minimum conditions for reclassification of a d...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question  \\\n",
       "0   1  For the treatment of cured defaulted exposures...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Source 1: CRR Article 178 > Default of an obli...   \n",
       "\n",
       "                                              output  \\\n",
       "0  Yes, a probation period of 90 days with no def...   \n",
       "\n",
       "                                            expected  rating  \n",
       "0  Minimum conditions for reclassification of a d...       9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ebb39-27fc-455d-90c7-668efa0c9525",
   "metadata": {},
   "source": [
    "### D. Show selected answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5fb769-21ab-4abb-a456-5c4066e4024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### 1. Question ######### \n",
      "For the treatment of cured defaulted exposures, a probation period of 90 days with no default triggers \n",
      "must apply before the exposure is moved back to a non-defaulted status. According to Article 178(1)(b CRR) \n",
      "default shall be considered to have occurred with regard to a particular obligor when the obligor is more \n",
      "than 90 days past due on any material credit obligation. However, if the material arrears fall below the \n",
      "thresholds, the arrears counter will reset to 0. Should the probation period of 90 days with no default \n",
      "triggers apply before the exposure is moved back to a non-defaulted status?\n",
      "\n",
      "\n",
      "######### 2. OpenAI answer ######### \n",
      "Yes, a probation period of 90 days with no default triggers should apply before a cured defaulted\n",
      "exposure is moved back to a non-defaulted status. According to Article 178 of the Capital\n",
      "Requirements Regulation (CRR), a default is recognized when an obligor is over 90 days past due on\n",
      "any material credit obligation, among other criteria. To revert to a non-defaulted status, competent\n",
      "guidelines suggest a 3-month (or 90-day) period should elapse without any default triggers\n",
      "continuing to apply to ensure the improvement in credit quality is genuine and lasting. This\n",
      "probation period aims to verify the obligor's financial stability and reliable behavior before\n",
      "reconfirming their status as non-defaulted.\n",
      "\n",
      "######### 3. EBA answer ######### \n",
      "Minimum conditions for reclassification of a defaulted exposure or obligor to a non-defaulted status, \n",
      "for the purposes of the application of Article 178(5) of Regulation (EU) 575/2013 (CRR), are specified \n",
      "in the Guidelines on the application of the definition of default under Article 178 of Regulation (EU) \n",
      "No 575/2013 (EBA/GL/2016/07).\n",
      "\n",
      "Except for situations referred to in paragraph 72 of said Guidelines (distressed restructuring on a \n",
      "defaulted exposure is subject to a probation period of at least one year â€“ see also Q&A 4867), all \n",
      "conditions from paragraph 71 of such Guidelines should be satisfied for the reclassification of a \n",
      "defaulted exposure or obligor to a non-defaulted status.\n",
      "\n",
      "As no trigger of default should continue to apply during a 3-months minimum probation period in accordance \n",
      "with point (a) of paragraph 71 of the Guidelines, no indication of unlikeliness to pay should be met in \n",
      "accordance with Article 178(1)(a) CRR and no material amount should be past due in accordance with point \n",
      "(b) of Article 178(1) CRR during this period.\n",
      "\n",
      "Furthermore, as part of the assessment required by point (d)  (d) of paragraph 71 of the Guidelines, \n",
      "point (b) of that paragraph paragraph  required that the institution takes into account the behavior \n",
      "of the obligor during this probation period before reclassifying a defaulted exposure to non-defaulted status, \n",
      "and thus the institution should consider any  past due amount during this period in the analysis of the \n",
      "behavior of the obligor.\n"
     ]
    }
   ],
   "source": [
    "# Show individual answers\n",
    "id = 0\n",
    "\n",
    "# Display a question\n",
    "print(\"######### 1. Question ######### \")\n",
    "print(df.at[id, 'question'].replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "# Display an OpenAI answer\n",
    "print(\"######### 2. OpenAI answer ######### \")\n",
    "print(df.at[id, 'output'].replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "# Display EBA answer\n",
    "print(\"\\n######### 3. EBA answer ######### \")\n",
    "print(df.at[id, 'expected'].replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32236490-5e2b-4002-938e-577dffff6cdf",
   "metadata": {},
   "source": [
    "### E. Additional info on evaluating answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ab6ec-5603-457e-81b2-39ffcdfa4ef0",
   "metadata": {},
   "source": [
    "https://github.com/redhat-et/foundation-models-for-documentation/blob/master/notebooks/llm-evaluation/QA_evaluation_metrics_demo.ipynb\n",
    "\n",
    "**Human Evaluation**\n",
    "Human evaluation is a widely recognized approach for assessing the quality of generated answers in comparison to real ones. This paper highlights some current trends and best practice guidelines. Here are some steps to summarize the process,\n",
    "\n",
    "**Best Practices for Human Evaluation Planning:**\n",
    "\n",
    "Define the evaluation goal: Clearly articulate the research question and determine if there are specific hypotheses to test. Choose strong and representative baselines for comparison.\n",
    "Determine the type of evaluation: Decide whether the evaluation will be intrinsic or extrinsic, and consider the real-world or lab setting based on the goals and constraints.\n",
    "Choose the type of research: Opt for qualitative research to improve the system or quantitative research to assess the system's merit.\n",
    "Define constructs of interest: Decide whether to ask implementation questions or impact questions. Use separate criteria instead of an overall text quality construct. Provide formal definitions and concrete examples of the criteria in the instructions.\n",
    "Determine appropriate scales: For quantitative research, consider using multiple-item 7-point Likert scales or a ranking task to measure participant responses.\n",
    "Determine the sample: Recruit participants that reflect the target audience and provide a detailed description of their demographics. Use large-scale samples for quantitative research and calculate the minimum sample size required. Consider using multiple annotators for coding tasks.\n",
    "Specify the study's design: Prefer a within-subjects design over a between-subjects design if feasible. Keep the evaluation task simple and motivating, reduce practice and carryover effects, manage fatigue and order effects, and address nonresponse bias.\n",
    "Select a statistical approach: Use exploratory data analysis techniques for exploratory research, and employ statistical significance testing and report effect sizes when there are clear hypotheses.\n",
    "Optional: Consider preregistering the task if the evaluation is confirmatory.\n",
    "These recommendations provide guidance for planning human evaluations and ensuring robust and meaningful results.\n",
    "\n",
    "**While it offers valuable insights, there are several challenges associated with this method.**\n",
    "\n",
    "Subjectivity: Human judgments can be subjective, leading to inconsistencies in the evaluation process.\n",
    "Inter-rater agreement: Ensuring agreement among evaluators becomes crucial to minimize biases and maintain reliability.\n",
    "Scalability: Evaluating a large number of generated answers manually becomes impractical, requiring sampling techniques or statistical methods.\n",
    "Expertise and domain knowledge: Evaluators' expertise and knowledge can influence evaluation outcomes, necessitating clear guidelines and appropriate training.\n",
    "Cost and time: Conducting human evaluations can be costly and time-consuming, requiring resources for recruitment, compensation, and management.\n",
    "Biases: Evaluators may have personal preferences or biases that can impact the evaluation results.\n",
    "Automatic metrics, including BLEU scores, ROUGE scores, and others mentioned earlier, have been observed to have limited correlation with human evaluations when it comes to evaluating generated text (reference). Critics argue against relying on automated metrics for assessing linguistic properties and discourage their primary use. However, there are still benefits to utilizing automatic metrics in terms of cost-effectiveness, speed, and repeatability, which make them valuable for tasks like error analysis and system development. Although human evaluation is widely considered the gold standard for assessing overall system quality, conducting it extensively throughout the development process can be expensive and time-consuming.\n",
    "\n",
    "**Importance about Prompt**\n",
    "\n",
    "When evaluating the answers generated by a language model, it is crucial to consider the quality of the question or prompt provided to the model. The performance of language models heavily relies on the input they receive, and a well-crafted prompt can significantly influence their output. A good prompt provides clear instructions, includes relevant context, and specifies the desired format or type of response. It helps guide the language model towards generating accurate and coherent answers. Therefore, it is essential to pay attention to both the quality of the generated answers and the quality of the prompts used during evaluation to obtain reliable and meaningful results. By understanding the impact of prompts on language model performance, we can improve the effectiveness of evaluations and enhance the overall performance of language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
